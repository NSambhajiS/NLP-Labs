{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uqa9ajwr1ph70Z6SIDKSKktGtZUvJoI2","timestamp":1741235478216}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Create a transformer from scratch using the Pytorch library **"],"metadata":{"id":"8j0-sCUDsFTp"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"UCEJiAk6kDbs","executionInfo":{"status":"ok","timestamp":1741667162832,"user_tz":-330,"elapsed":6518,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"outputs":[],"source":["#build a text classification model using a Transformer architecture.\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np"]},{"cell_type":"code","source":["# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGMbFfoakT50","outputId":"651d4727-a968-48a2-cfb7-40265e014d17","executionInfo":{"status":"ok","timestamp":1741667162889,"user_tz":-330,"elapsed":73,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# Sample dataset (replace with real dataset)\n","data = [\n","    (\"The cat sat on the mat\", 0),\n","    (\"Dogs are loyal animals\", 1),\n","    (\"Transformers are powerful models\", 1),\n","    (\"Pytorch makes deep learning easy\", 0),\n","]\n","vocab = list(set(word for sentence, _ in data for word in sentence.split()))\n","word2idx = {word: idx for idx, word in enumerate(vocab)} #word2idx dictionary is created to map words to numerical indices."],"metadata":{"id":"2X9oYopVkZxq","executionInfo":{"status":"ok","timestamp":1741667162897,"user_tz":-330,"elapsed":6,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Convert dataset into token indices\n","class TextDataset(Dataset):\n","    def __init__(self, data, word2idx, max_len=None):  # Add max_len parameter\n","        self.data = [(torch.tensor([word2idx[word] for word in sentence.split()], dtype=torch.long), label)\n","                     for sentence, label in data]\n","        self.max_len = max_len if max_len else max(len(x[0]) for x in self.data)  # Calculate or use provided max_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # Pad sequences to max_len\n","        tensor, label = self.data[idx]\n","        padded_tensor = torch.zeros(self.max_len, dtype=torch.long)\n","        padded_tensor[:len(tensor)] = tensor\n","        return padded_tensor, label  # Return padded tensor\n","\n","dataset = TextDataset(data, word2idx)\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"],"metadata":{"id":"mYpMgQjukdqd","executionInfo":{"status":"ok","timestamp":1741667162940,"user_tz":-330,"elapsed":16,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Transformer Model Definition\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, num_classes):\n","        super(TransformerModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        self.fc = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)  # Shape: (batch_size, seq_len, embed_dim)\n","        x = x.permute(1, 0, 2)  # Required shape for Transformer (seq_len, batch_size, embed_dim)\n","        x = self.transformer_encoder(x)  # Apply transformer\n","        x = x.mean(dim=0)  # Global Average Pooling\n","        x = self.fc(x)  # Fully connected layer\n","        return x"],"metadata":{"id":"nFR860UkkhDa","executionInfo":{"status":"ok","timestamp":1741667163233,"user_tz":-330,"elapsed":265,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","vocab_size = len(vocab)\n","embed_dim = 32\n","num_heads = 2\n","num_layers = 2\n","hidden_dim = 64\n","num_classes = 2"],"metadata":{"id":"kThWObD9kmXu","executionInfo":{"status":"ok","timestamp":1741667163236,"user_tz":-330,"elapsed":17,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Model, Loss, Optimizer\n","model = TransformerModel(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqxh54KtkpcE","outputId":"4272fc4b-8eb1-4597-8490-7c7ee121ce93","executionInfo":{"status":"ok","timestamp":1741667174111,"user_tz":-330,"elapsed":10886,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Training Loop\n","def train_model(model, dataloader, criterion, optimizer, epochs=10):\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n","\n","train_model(model, dataloader, criterion, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBbTYhoaktkV","outputId":"88fd8c0d-0c7c-448d-9b45-456c302c8b2d","executionInfo":{"status":"ok","timestamp":1741667175218,"user_tz":-330,"elapsed":1099,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 1.3434\n","Epoch 2/10, Loss: 1.0821\n","Epoch 3/10, Loss: 0.9728\n","Epoch 4/10, Loss: 0.7832\n","Epoch 5/10, Loss: 0.6502\n","Epoch 6/10, Loss: 0.5789\n","Epoch 7/10, Loss: 0.4295\n","Epoch 8/10, Loss: 0.3740\n","Epoch 9/10, Loss: 0.3070\n","Epoch 10/10, Loss: 0.2494\n"]}]},{"cell_type":"code","source":["# Save Model\n","torch.save(model.state_dict(), \"transformer_model.pth\")\n","print(\"Model saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6ZGFmbalIHd","outputId":"38ce3d03-02e6-4e8e-ed22-ff087de13bf4","executionInfo":{"status":"ok","timestamp":1741667175253,"user_tz":-330,"elapsed":29,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved successfully!\n"]}]}]}
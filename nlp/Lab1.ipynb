{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQmzDj0ZclNvTbqaee/JX2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Tokenization, stemming and lemitization"],"metadata":{"id":"Cejit8ETlsYb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfUt0LDviJaE"},"outputs":[],"source":["import nltk   #Natural Language Toolkit\n","from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer\n","from nltk.tokenize.mwe import MWETokenizer\n","from nltk.stem import PorterStemmer, SnowballStemmer\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","source":["# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5ZWLOvNi7Hg","executionInfo":{"status":"ok","timestamp":1738128801173,"user_tz":-330,"elapsed":3788,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}},"outputId":"4999dd40-4ecd-477f-84c7-5c98a67ae405"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Sample text\n","text = \"NLTK is a powerful library for NLP! It's easy-to-use and supports multiple tokenization techniques.\"\n"],"metadata":{"id":"EKbZRhKVi9r0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenization\n","whitespace_tokenizer = WhitespaceTokenizer()\n","punct_tokenizer = WordPunctTokenizer()\n","treebank_tokenizer = TreebankWordTokenizer()\n","tweet_tokenizer = TweetTokenizer()\n","mwe_tokenizer = MWETokenizer([(\"easy\", \"to\", \"use\")])"],"metadata":{"id":"LhlBHWdKjAIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Whitespace Tokenization:\", whitespace_tokenizer.tokenize(text))\n","print(\"Punctuation-based Tokenization:\", punct_tokenizer.tokenize(text))\n","print(\"Treebank Tokenization:\", treebank_tokenizer.tokenize(text))\n","print(\"Tweet Tokenization:\", tweet_tokenizer.tokenize(text))\n","print(\"MWE Tokenization:\", mwe_tokenizer.tokenize(text.split()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PtbfNmyjC3A","executionInfo":{"status":"ok","timestamp":1738128814135,"user_tz":-330,"elapsed":17,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}},"outputId":"79039f7c-9fc6-44db-e83d-0c893e8b5abd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Whitespace Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP!', \"It's\", 'easy-to-use', 'and', 'supports', 'multiple', 'tokenization', 'techniques.']\n","Punctuation-based Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '!', 'It', \"'\", 's', 'easy', '-', 'to', '-', 'use', 'and', 'supports', 'multiple', 'tokenization', 'techniques', '.']\n","Treebank Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '!', 'It', \"'s\", 'easy-to-use', 'and', 'supports', 'multiple', 'tokenization', 'techniques', '.']\n","Tweet Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '!', \"It's\", 'easy-to-use', 'and', 'supports', 'multiple', 'tokenization', 'techniques', '.']\n","MWE Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP!', \"It's\", 'easy-to-use', 'and', 'supports', 'multiple', 'tokenization', 'techniques.']\n"]}]},{"cell_type":"code","source":["# Stemming\n","porter = PorterStemmer()\n","snowball = SnowballStemmer(\"english\")\n","\n","words = [\"running\", \"flies\", \"easily\", \"studies\"]\n","print(\"Porter Stemmer:\", [porter.stem(word) for word in words])\n","print(\"Snowball Stemmer:\", [snowball.stem(word) for word in words])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TG1e4NcFjGDO","executionInfo":{"status":"ok","timestamp":1738128817758,"user_tz":-330,"elapsed":831,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}},"outputId":"689e573d-1c8f-4231-fb31-08647daa7338"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Porter Stemmer: ['run', 'fli', 'easili', 'studi']\n","Snowball Stemmer: ['run', 'fli', 'easili', 'studi']\n"]}]},{"cell_type":"code","source":["# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","print(\"Lemmatization:\", [lemmatizer.lemmatize(word) for word in words])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ms6aFsBsjGny","executionInfo":{"status":"ok","timestamp":1738128824007,"user_tz":-330,"elapsed":3906,"user":{"displayName":"Neha Sawant","userId":"08732833319081881412"}},"outputId":"83adedf1-b43f-4dfd-f269-5cc23d8aaf3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemmatization: ['running', 'fly', 'easily', 'study']\n"]}]}]}